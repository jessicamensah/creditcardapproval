# -*- coding: utf-8 -*-
"""CMP7228 Machine Learning 1

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SGmWhVTWsD9Lcr2hS59B3LdV_aIISGo0

Cleaning Data
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import sklearn
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import seaborn as sns
# %matplotlib inline

"""Importing the CSV as raw data into Colab"""

df = pd.read_csv('/content/application_record.csv')

"""Converting the labels from days to years and rounding the numbers for normalisation purposes"""

df['DAYS_BIRTH'] = round(abs(df['DAYS_BIRTH']) / 365) # divide the days of birth by 365 to convert to years and 'abs' to convert into a positive number
df['DAYS_EMPLOYED'] = np.where(df['DAYS_EMPLOYED']==365243, 0, df['DAYS_EMPLOYED']) # replacing the values of 365243 with 0 = havent worked
df['DAYS_EMPLOYED'] = round(abs(df['DAYS_EMPLOYED']) / 365) # divide the days of employment by 365 to convert to years and 'abs' to convert into a positive number

"""Filling the IsNull Values with 'missing'"""

df.isna().sum()

# I treated the missing values
df['OCCUPATION_TYPE'] = df['OCCUPATION_TYPE'].fillna('missing')

df.head()

df.shape

"""Label Encoding the data to normalise it"""

# Label Encode the data
cols = ['CODE_GENDER','FLAG_OWN_CAR','FLAG_OWN_REALTY','NAME_INCOME_TYPE','NAME_EDUCATION_TYPE','NAME_FAMILY_STATUS',
'NAME_HOUSING_TYPE','OCCUPATION_TYPE']

for i in cols:
  label_encoder = LabelEncoder()
  df[i] = label_encoder.fit_transform(df[i])

df.head()

'''Dropping unnecessery columns'''

df.drop(['ID', 'FLAG_MOBIL', 'FLAG_WORK_PHONE', 'FLAG_PHONE', 'FLAG_EMAIL'],axis = 1, inplace = True)

"""Fixing The Outliers"""

upper_limit_child = df['CNT_CHILDREN'].mean() + 3 * df['CNT_CHILDREN'].std()
upper_limit_income = df['AMT_INCOME_TOTAL'].mean() + 2.2 * df['AMT_INCOME_TOTAL'].std()
upper_limit_fam_members = df['CNT_FAM_MEMBERS'].mean() + 3 * df['CNT_FAM_MEMBERS'].std()
upper_limit_days_employed = df['DAYS_EMPLOYED'].mean() + 1.9 * df['DAYS_EMPLOYED'].std()

df = df.loc[df['CNT_CHILDREN'] <= upper_limit_child]
df = df.loc[df['AMT_INCOME_TOTAL'] <= upper_limit_income]
df = df.loc[df['CNT_FAM_MEMBERS'] <= upper_limit_fam_members]
df = df.loc[df['DAYS_EMPLOYED'] <= upper_limit_days_employed]

df

"""Visualising the Data"""

sns.boxenplot(x='AMT_INCOME_TOTAL', data=df)

df.head()

sns.scatterplot(x='DAYS_EMPLOYED', y='AMT_INCOME_TOTAL', data=df)

from mpl_toolkits.mplot3d import Axes3D

# Assuming df is your dataframe
fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')

# Scatter plot with color based on 'CODE_GENDER'
scatter = ax.scatter(df['AMT_INCOME_TOTAL'], df['DAYS_EMPLOYED'], df['CNT_CHILDREN'], s=20, c=df['NAME_INCOME_TYPE'], cmap='viridis', marker='o')

# Set labels
ax.set_xlabel('AMT_INCOME_TOTAL')
ax.set_ylabel('DAYS_EMPLOYED')
ax.set_zlabel('CNT_CHILDREN')

# Add color bar
cbar = plt.colorbar(scatter, ax=ax)
cbar.set_label('NAME_INCOME_TYPE')

# Show the plot
plt.show()

"""The data below is right skued meaning that there are many outliers"""

sns.barplot(x='CODE_GENDER', y='FLAG_OWN_CAR', data=df)

sns.heatmap(df.corr(), annot=True)

sns.displot(df['AMT_INCOME_TOTAL'], kde=True, bins=15)

"""Proof of outliers z-score method"""

sns.kdeplot(data=df, x='AMT_INCOME_TOTAL', hue='CODE_GENDER', multiple='stack', common_norm=False, common_grid=True, fill=True)

fig, ax1 = plt.subplots(figsize=(10, 6))

# Plot KDE on the left y-axis
sns.kdeplot(data=df, x='DAYS_EMPLOYED', hue='CODE_GENDER', multiple='stack', common_norm=False, common_grid=True, fill=True, ax=ax1)

# Create a second y-axis for the histogram on the right
ax2 = ax1.twinx()
sns.histplot(x='DAYS_EMPLOYED', data=df, hue='CODE_GENDER', ax=ax2, alpha=0.5)

df.head()

"""Testing, Training and Splitting the data"""

# split data into X and y
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
import joblib

X = df[['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY', 'CNT_CHILDREN', 'NAME_INCOME_TYPE', 'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'CNT_FAM_MEMBERS','OCCUPATION_TYPE']]
Y = df['AMT_INCOME_TOTAL'] # target variable

X_train, X_test, y_train, y_test = train_test_split( X, Y, test_size=0.3, random_state=42)

print(len(X_train))
print(len(X_test))

# Normalize the Data
scaler = StandardScaler()
X_train_nom = scaler.fit_transform(X_train)
X_test_nom = scaler.transform(X_test)

X_train_nom

"""The fitting and Prediction of the test and training data"""

# Building Linear Regression Model
lr_model = LinearRegression()
lr_model.fit(X_train_nom, y_train)

# Predict on the test
y_pred = lr_model.predict(X_test_nom)

# Evaluate Linear Model
from sklearn.metrics import *

mean_squared_error(y_pred, y_test)

"""Linear Regression Outcome as a Scatter plot"""

import seaborn as sns

sns.regplot(x=y_test, y=y_pred)
plt.xlabel('Income Amount')
plt.ylabel('Density')
plt.title('Values vs Income')
plt.show()

"""Linear Regression as a Bar Plot regarding its Feature Importance"""

plt.barh(X.columns, lr_model.coef_[0], color='red')
plt.xlabel('Feature Importance')
plt.ylabel('Features')
plt.title('Linear Regression Feature Importance')
plt.show()

"""Fit and Prediction of Model"""

from xgboost import XGBRegressor

xgb_model = XGBRegressor()
xgb_model.fit(X_train_nom, y_train)

# Predict on the test
xgb_y_pred = xgb_model.predict(X_test_nom)

"""Finding Mean Squared Error (accuracy score)"""

# Evaluate XGB Model
from sklearn.metrics import *

mean_squared_error(xgb_y_pred, y_test)

"""Identifying the Feature Impirtance when XGBoost is used"""

# Plotting the feature importance for XGBoost
plt.figure(figsize=(10, 6))
plt.barh(X.columns, xgb_model.feature_importances_, color='ORANGE')
plt.xlabel('Feature Importance')
plt.ylabel('Features')
plt.title('XGBoost Feature Importance')
plt.show()

"""Scatterplot with lines of the income categories"""

import seaborn as sns

income_ranges = [(0, 50000), (50000, 100000), (100000, 150000), (150000, 200000), (200000, 250000), (250000, 300000), (300000, 350000), (350000, 400000), (400000, 450000)]  # Define income ranges
for income_range in income_ranges:
    plt.axvspan(income_range[0], income_range[1], alpha=0.3, color='gray')

sns.regplot(x=y_test, y=xgb_y_pred)
plt.xlabel('Income Amount')
plt.ylabel('Density')
plt.title('Values')
plt.show()

"""Train, Test Split of Decision Tree"""

from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, plot_tree

X = df[['CODE_GENDER', 'FLAG_OWN_CAR', 'AMT_INCOME_TOTAL', 'CNT_CHILDREN', 'NAME_INCOME_TYPE', 'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'CNT_FAM_MEMBERS','OCCUPATION_TYPE']] #other features
y = df['FLAG_OWN_REALTY'] # target variable

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

"""Fit, Transform and Prediction of the Model"""

# Normalize the Data
scaler = StandardScaler()
X_train_nom = scaler.fit_transform(X_train)
X_test_nom = scaler.transform(X_test)

dt_model = DecisionTreeClassifier(max_depth=6)
dt_model.fit(X_train_nom, y_train)

y_pred_dt = dt_model.predict(X_test_nom)

"""Testing of Accuracy Score"""

accuracy_score(y_pred_dt, y_test)

"""Decision Tree Output"""

plt.figure(figsize=(15, 10))
plot_tree(dt_model, filled=True, feature_names=X.columns, rounded=True)
plt.show()

"""Random Forest Fit and Prediction"""

from sklearn.ensemble import RandomForestClassifier

rf_model = RandomForestClassifier()
rf_model.fit(X_train_nom, y_train)

y_pred_rf = rf_model.predict(X_test_nom)

"""Checking for the Accuracy Score of Model"""

accuracy_score(y_pred_rf, y_test)

"""Checking for Decision Tree changes"""

plt.figure(figsize=(15, 10))
plot_tree(dt_model, filled=True, feature_names=X.columns, rounded=True)
plt.show()

"""Checking for Feature Importance with Random Forest"""

# Plotting the feature importance for RandomForest
plt.figure(figsize=(10, 6))
plt.barh(X.columns, rf_model.feature_importances_, color='PINK')
plt.xlabel('Feature Importance')
plt.ylabel('Features')
plt.title('RandomForest Feature Importance')
plt.show() #if your income is high, you will own a house

"""Hierachal Clustering"""

import scipy.cluster.hierarchy as sch
from sklearn.metrics import silhouette_score
from sklearn.datasets import make_blobs
import matplotlib.pyplot as plt

"""Characteristics of the Hierarchal Cluster being added below"""

df = make_blobs(n_samples = 200,
                n_features = 2,
                centers = 2,
                cluster_std = 1.6,
                random_state = 50)

points = df[0]

from sklearn.cluster import AgglomerativeClustering

"""Hierarchal Clustering Outcome"""

dendogram = sch.dendrogram(sch.linkage(points, method='ward'))

plt.scatter(df[0][:,0], df[0][:,1])

hc = AgglomerativeClustering(n_clusters=4, affinity='euclidean', linkage='ward')

y_hc = hc.fit_predict(points)

"""Hierarchal Clustering as a scatter plot"""

plt.scatter(points[y_hc == 0, 0], points[y_hc == 0, 1], s=100, c= 'blue')
plt.scatter(points[y_hc == 1, 0], points[y_hc == 1, 1], s=100, c= 'red')
plt.scatter(points[y_hc == 2, 0], points[y_hc == 2, 1], s=100, c= 'yellow')
plt.scatter(points[y_hc == 3, 0], points[y_hc == 3, 1], s=100, c= 'green')

"""Silhouette Score to check the accuracy of the model"""

silhouette_avg = silhouette_score(points, y_hc)
print("Silhouette Score:", silhouette_avg)

plt.show()

"""Hierarchal Clustering with 2 clusters"""

from sklearn.datasets import make_blobs
import matplotlib.pyplot as plt

# Create 2 clusters
df = make_blobs(n_samples=200,
                n_features=2,
                centers=2,
                cluster_std=1.6,
                random_state=50)

hc2 = AgglomerativeClustering(n_clusters=4, affinity='euclidean', linkage='ward')
y_hc2 = hc.fit_predict(points)

# Scatter plot to visualize the clusters
plt.scatter(df[0][:, 0], df[0][:, 1], c=df[1], cmap='viridis')
plt.title('Generated Data with 2 Clusters')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.show()

silhouette_avg = silhouette_score(points, y_hc2)
print("Silhouette Score:", silhouette_avg)

plt.show()

"""K Means Clustering"""

df = make_blobs(n_samples = 200,
                n_features = 2,
                centers = 4,
                cluster_std = 1.6,
                random_state = 50)

from sklearn.cluster import KMeans

kmeans = KMeans(n_clusters=4)

kmeans.fit(points)

plt.scatter(df[0][:,0], df[0][:,1])

clusters = kmeans.cluster_centers_

print(clusters)

y_km = kmeans.fit_predict(points)

"""Scatterplot for KMeans"""

plt.scatter(points[y_km == 0, 0], points[y_km == 0, 1], s=100, c='yellow', label='Customer type 0')
plt.scatter(points[y_km == 1, 0], points[y_km == 1, 1], s=100, c='red', label='Customer type 1')
plt.scatter(points[y_km == 2, 0], points[y_km == 2, 1], s=100, c='pink', label='Customer type 2')
plt.scatter(points[y_km == 3, 0], points[y_km == 3, 1], s=100, c='cyan', label='Customer type 3')


plt.scatter(clusters[0][0], clusters[0][1], marker='*', s=200, c= 'black')
plt.scatter(clusters[1][0], clusters[1][1], marker='*', s=200, c= 'black')
plt.scatter(clusters[2][0], clusters[2][1], marker='*', s=200, c= 'black')
plt.scatter(clusters[3][0], clusters[3][1], marker='*', s=200, c= 'black')

silhouette_avg = silhouette_score(points, y_km)
print("Silhouette Score:", silhouette_avg)

plt.show()